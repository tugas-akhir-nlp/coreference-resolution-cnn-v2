{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-56117c201f7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtabulate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.data_helper import get_markable_dataframe, get_embedding_variables, get_sentence_variables, get_document_id_variables, get_phrases_and_nodes\n",
    "from model_builders.coreference_classifier import CoreferenceClassifierModelBuilder\n",
    "from functools import reduce\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from utils.clusterers import BestFirstClusterer, get_anaphora_scores_by_antecedent, ClosestFirstClusterer\n",
    "from utils.scorers import MUCScorer, B3Scorer, CEAFeScorer, AverageScorer\n",
    "from utils.data_structures import UFDS\n",
    "from xml.etree import ElementTree\n",
    "from string import punctuation\n",
    "from IPython.display import HTML, display\n",
    "from tabulate import tabulate\n",
    "from html import escape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.read_csv(\"data/testing/mention_pairs.csv\")\n",
    "\n",
    "label = np.vstack(to_categorical(pairs.is_coreference, num_classes=2))\n",
    "label_chains = ClosestFirstClusterer().get_chains(get_anaphora_scores_by_antecedent(pairs.m1_id, pairs.m2_id, label))\n",
    "# label_chains = sorted(list(filter(lambda x: len(x) > 1, label_chains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_id_by_markable_id, markable_ids_by_sentence_id = get_sentence_variables('data/full.xml')\n",
    "document_id_by_sentence_id, document_id_by_markable_id, sentence_ids_by_document_id, markable_ids_by_document_id = get_document_id_variables('data/document_id.csv', markable_ids_by_sentence_id)\n",
    "\n",
    "data = ElementTree.parse('data/full.xml')\n",
    "\n",
    "root = data.getroot()\n",
    "parent_map = {c: p for p in root.iter() for c in p}\n",
    "\n",
    "phrases, nodes, phrase_id_by_node_id = get_phrases_and_nodes(UFDS(), root)\n",
    "\n",
    "phrases_by_sentence_id = {}\n",
    "\n",
    "aneh = []\n",
    "for phrase in phrases:\n",
    "    sentence = parent_map[phrase]\n",
    "    sentence_id = int(sentence.attrib['id'])\n",
    "    \n",
    "    if sentence_id not in phrases_by_sentence_id:\n",
    "        phrases_by_sentence_id[sentence_id] = []\n",
    "    \n",
    "    if 'coref' in phrase.attrib:\n",
    "        if document_id_by_markable_id[int(phrase.attrib['coref'])] != document_id_by_sentence_id[sentence_id]:\n",
    "            aneh.append(document_id_by_markable_id[int(phrase.attrib['coref'])])\n",
    "            aneh.append(document_id_by_sentence_id[sentence_id])\n",
    "            \n",
    "    phrases_by_sentence_id[sentence_id].append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUC:  (0.6395348837209303, 0.7051282051282052, 0.6707317073170733)\n",
      "B3:  (0.5041087231352718, 0.6379818594104308, 0.5631991462778547)\n",
      "Average:  (0.4185637051277797, 0.4185637051277797, 0.41856370512777963)\n"
     ]
    }
   ],
   "source": [
    "baseline_result_file_path = 'baseline/suherik_and_purwarianti/test_result.txt'\n",
    "\n",
    "baseline_ufds = UFDS()\n",
    "\n",
    "for m1, m2 in zip(pairs.m1_id, pairs.m2_id):\n",
    "    baseline_ufds.init_id(m1, m2)\n",
    "    \n",
    "for line in open(baseline_result_file_path, 'r').readlines():\n",
    "    line = line.split(', ')\n",
    "    m1_id, m2_id = int(line[0]), int(line[1])\n",
    "    \n",
    "    if document_id_by_markable_id[m1_id] == document_id_by_markable_id[m2_id]:\n",
    "        baseline_ufds.join(m1_id, m2_id)\n",
    "\n",
    "baseline_chains = baseline_ufds.get_chain_list()\n",
    "\n",
    "print('MUC: ', MUCScorer().get_scores(baseline_chains, label_chains))\n",
    "print('B3: ', B3Scorer().get_scores(baseline_chains, label_chains))\n",
    "print('Average: ', AverageScorer([MUCScorer(), B3Scorer(), CEAFeScorer()]).get_scores(baseline_chains, label_chains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ElementTree.parse('data/testing/data.xml')\n",
    "\n",
    "root = data.getroot()\n",
    "\n",
    "test_document_ids = set()\n",
    "for sentence in root:\n",
    "    test_document_ids.add(document_id_by_sentence_id[int(sentence.attrib['id'])])\n",
    "\n",
    "test_document_ids = list(test_document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_without_pos_tag(text):\n",
    "    words = text.split()\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        words[i] = ''.join(words[i].split('\\\\')[:-1])\n",
    "    \n",
    "    return ' '.join(words)\n",
    "    \n",
    "def get_sentence(sentence_id):\n",
    "    phrases = phrases_by_sentence_id[sentence_id]\n",
    "    \n",
    "    splitted_sentence = []\n",
    "    for phrase in phrases:\n",
    "        text = get_text_without_pos_tag(phrase.text)\n",
    "        \n",
    "        if 'id' in phrase.attrib:\n",
    "            splitted_sentence.append(f'({text})[{phrase.attrib[\"id\"]}]')\n",
    "        else:\n",
    "            splitted_sentence.append(text)\n",
    "            \n",
    "    sentence = ' '.join(splitted_sentence)\n",
    "    return sentence\n",
    "\n",
    "def get_document_text(document_id):\n",
    "    sentence_ids = sentence_ids_by_document_id[document_id]\n",
    "    sentences = [get_sentence(sentence_id) for sentence_id in sentence_ids]\n",
    "    \n",
    "    document_text = ''\n",
    "    for sentence in sentences:\n",
    "        document_text += sentence\n",
    "        document_text += ' ' if sentence[-1] in punctuation else '\\n'\n",
    "        \n",
    "    return document_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Baseline Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_result_file_path = 'baseline/suherik_and_purwarianti/test_result.txt'\n",
    "\n",
    "baseline_ufds = UFDS()\n",
    "\n",
    "for m1, m2 in zip(pairs.m1_id, pairs.m2_id):\n",
    "    baseline_ufds.init_id(m1, m2)\n",
    "    \n",
    "for line in open(baseline_result_file_path, 'r').readlines():\n",
    "    line = line.split(', ')\n",
    "    baseline_ufds.join(int(line[0]), int(line[1]))\n",
    "\n",
    "baseline_chains = list(filter(lambda x: len(x) > 1, baseline_ufds.get_chain_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Predicted Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_result_wo_sc_file_path = 'result/without_singleton_classifier.json'\n",
    "predicted_result_w_sc_file_path = 'result/with_singleton_classifier.json'\n",
    "predicted_result_w_label_sc_file_path = 'result/with_label_singleton_classifier.json'\n",
    "\n",
    "wo_sc_chains = list(map(lambda chain: [markable[0] for markable in chain], json.load(open(predicted_result_wo_sc_file_path))))\n",
    "w_sc_chains = list(map(lambda chain: [markable[0] for markable in chain], json.load(open(predicted_result_w_sc_file_path))))\n",
    "w_label_sc_chains = list(map(lambda chain: [markable[0] for markable in chain], json.load(open(predicted_result_w_label_sc_file_path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chains_by_document_id(chains, document_id):\n",
    "    filtered_chains = []\n",
    "    \n",
    "    for chain in chains:\n",
    "        filtered_chain = [markable_id for markable_id in chain if document_id_by_markable_id[markable_id] == document_id]\n",
    "        if len(filtered_chain) > 1:\n",
    "            filtered_chains.append(filtered_chain)\n",
    "    \n",
    "    return filtered_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PT Astra Otoparts Tbk)[1916] menjual kepemilikan saham (nya)[1917] di (PT Exedy Indonesia)[1918] (sebanyak 7.072 saham)[1919] dengan (harga)[1920] (Rp 1.835.000)[1921] per (saham)[1922] atau total (nya)[1923] (senilai Rp 12,977 miliar)[1924] kepada (Exedy Corporation)[1925] yang berkedudukan di (Jepang.)[1926] (Kami)[1927] telah menandatangani (perjanjian pengikatan)[1928] untuk penjualan (seluruh saham)[1929] milik (PT Astra Otoparts)[1930] di (PT Exedy Indonesia)[1931] kepada (Exedy Corporation,)[1932] kata (Sekretaris Perusahaan Astra Otoparts,)[1933] (Kartina Rahayu)[1934] di (Jakarta,)[1935] (Senin.)[1936] Menurut (dia,)[1937] (penjualan saham tersebut)[1938] akan berlaku efektif apabila syarat-syarat sebagaimana termuat dalam (perjanjian pengikatan jual beli saham)[1939] telah terpenuhi. (Dia)[1940] mengatakan (transaksi tersebut)[1941] tidak mengandung (unsur benturan kepentingan)[1942] dilihat dari sisi (direksi,)[1943] (komisaris)[1944] dan (pemegang saham utama.)[1945] Lagi pula nilai (nya)[1946] (tidak cukup material)[1947] yakni (kurang 10 persen)[1948] dari (pendapatan)[1949] dan (20 persen)[1950] dari (ekuitas,)[1951] tambah (nya.)[1952] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5bcb12829b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_document_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     display(HTML(tabulate(\n\u001b[0m\u001b[1;32m     16\u001b[0m         [[get_printable_chains(get_chains_by_document_id(chains, document_id)) for chains in chains_list]], tablefmt='html')))\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    }
   ],
   "source": [
    "chains_list = [label_chains, baseline_chains, wo_sc_chains, w_sc_chains, w_label_sc_chains]\n",
    "\n",
    "def get_printable_chains(chains):\n",
    "    printable = ''\n",
    "    \n",
    "    for chain in chains:\n",
    "        printable += str(chain)\n",
    "        printable += '<br />'\n",
    "    \n",
    "    return printable\n",
    "    \n",
    "for document_id in test_document_ids:\n",
    "    print(get_document_text(document_id))\n",
    "    \n",
    "    display(HTML(tabulate(\n",
    "        [[get_printable_chains(get_chains_by_document_id(chains, document_id)) for chains in chains_list]], tablefmt='html')))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
