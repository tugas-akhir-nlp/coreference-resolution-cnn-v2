{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.data_helper import get_markable_dataframe, get_embedding_variables\n",
    "from model_builders.coreference_classifier import CoreferenceClassifierModelBuilder\n",
    "from functools import reduce\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_indexes_file_path = 'helper_files/embedding/embedding_indexes.txt'\n",
    "indexed_embedding_file_path = 'helper_files/embedding/indexed_embedding.txt'\n",
    "\n",
    "word_vector, embedding_matrix, idx_by_word, word_by_idx = get_embedding_variables(embedding_indexes_file_path, indexed_embedding_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_pronoun</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>is_proper_name</th>\n",
       "      <th>is_first_person</th>\n",
       "      <th>previous_words</th>\n",
       "      <th>next_words</th>\n",
       "      <th>is_singleton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 2]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[6, 7, 8, 9, 10, 2, 11, 12, 13, 14]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[6, 7, 8]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>[9, 10, 2, 11, 12, 13, 14, 15, 16, 17]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[10]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>[2, 11, 12, 13, 14, 15, 16, 17, 18, 19]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[12, 13]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, 4, 5, 6, 7, 8, 9, 10, 2, 11]</td>\n",
       "      <td>[14, 15, 16, 17, 18, 19, 4, 5, 20, 21]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       text  is_pronoun                     entity_type  is_proper_name  \\\n",
       "0   1        [1]           0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]               0   \n",
       "1   2     [4, 5]           0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]               0   \n",
       "2   3  [6, 7, 8]           0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 1]               1   \n",
       "3   4       [10]           0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]               0   \n",
       "4   5   [12, 13]           0  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]               0   \n",
       "\n",
       "   is_first_person                    previous_words  \\\n",
       "0                0                                []   \n",
       "1                0                         [1, 2, 3]   \n",
       "2                0                   [1, 2, 3, 4, 5]   \n",
       "3                0       [1, 2, 3, 4, 5, 6, 7, 8, 9]   \n",
       "4                0  [3, 4, 5, 6, 7, 8, 9, 10, 2, 11]   \n",
       "\n",
       "                                next_words is_singleton  \n",
       "0          [2, 3, 4, 5, 6, 7, 8, 9, 10, 2]   [0.0, 1.0]  \n",
       "1      [6, 7, 8, 9, 10, 2, 11, 12, 13, 14]   [0.0, 1.0]  \n",
       "2   [9, 10, 2, 11, 12, 13, 14, 15, 16, 17]   [1.0, 0.0]  \n",
       "3  [2, 11, 12, 13, 14, 15, 16, 17, 18, 19]   [0.0, 1.0]  \n",
       "4   [14, 15, 16, 17, 18, 19, 4, 5, 20, 21]   [0.0, 1.0]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markables = get_markable_dataframe(\"data/training/markables.csv\", word_vector, idx_by_word)\n",
    "\n",
    "markables.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1_id</th>\n",
       "      <th>m2_id</th>\n",
       "      <th>is_exact_match</th>\n",
       "      <th>is_words_match</th>\n",
       "      <th>is_substring</th>\n",
       "      <th>is_abbreviation</th>\n",
       "      <th>is_appositive</th>\n",
       "      <th>is_nearest_candidate</th>\n",
       "      <th>sentence_distance</th>\n",
       "      <th>word_distance</th>\n",
       "      <th>markable_distance</th>\n",
       "      <th>is_coreference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   m1_id  m2_id  is_exact_match  is_words_match  is_substring  \\\n",
       "0      3     11               0               0             0   \n",
       "1      4     11               0               0             0   \n",
       "2      5     11               0               0             0   \n",
       "3      6     11               0               0             0   \n",
       "4      7     11               0               0             0   \n",
       "\n",
       "   is_abbreviation  is_appositive  is_nearest_candidate  sentence_distance  \\\n",
       "0                0              0                     0                  2   \n",
       "1                0              0                     0                  2   \n",
       "2                0              0                     0                  2   \n",
       "3                0              0                     0                  2   \n",
       "4                0              0                     0                  1   \n",
       "\n",
       "   word_distance  markable_distance  is_coreference  \n",
       "0             31                  8               1  \n",
       "1             29                  7               0  \n",
       "2             25                  6               0  \n",
       "3             16                  5               0  \n",
       "4             14                  4               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_budi = pd.read_csv(\"data/training/mention_pairs_budi.csv\")\n",
    "pairs_soon = pd.read_csv(\"data/training/mention_pairs_soon.csv\")\n",
    "pairs_gilang = pd.read_csv(\"data/training/mention_pairs_gilang.csv\")\n",
    "\n",
    "pairs_soon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_length = 10\n",
    "max_prev_words_length = 10\n",
    "max_next_words_length = 10\n",
    "\n",
    "def get_data(markable_ids):\n",
    "    indices = reduce(lambda a, b: a + [b], map(lambda a: markables.index[markables['id'] == a].tolist()[0], markable_ids), [])\n",
    "    data = markables.loc[indices]\n",
    "    \n",
    "    data_text = pad_sequences(data.text, maxlen=max_text_length, padding='post')\n",
    "    data_previous_words = pad_sequences(data.previous_words.map(lambda seq: seq[(-1*max_prev_words_length):]), maxlen=max_prev_words_length, padding='pre')\n",
    "    data_next_words = pad_sequences(data.next_words.map(lambda seq: seq[:max_next_words_length]), maxlen=max_next_words_length, padding='post')\n",
    "    data_syntactic = data[['is_pronoun', 'entity_type', 'is_proper_name', 'is_first_person']]\n",
    "\n",
    "    data_syntactic = np.array(list(map(lambda p: reduce(lambda x,y: x + y, [i if type(i) is list else [i] for i in p]), data_syntactic.values)))\n",
    "    is_singleton = np.vstack(data.is_singleton)\n",
    "    \n",
    "    return data_text, data_previous_words, data_next_words, data_syntactic, is_singleton\n",
    "\n",
    "def get_pair_data(markable_ids_1, markable_ids_2):\n",
    "    text_1, prev_1, next_1, syntactic_1, is_singleton_1 = get_data(markable_ids_1)\n",
    "    text_2, prev_2, next_2, syntactic_2, is_singleton_2 = get_data(markable_ids_2)\n",
    "    \n",
    "    return text_1, text_2, prev_1, prev_2, next_1, next_2, syntactic_1, syntactic_2, is_singleton_1, is_singleton_2\n",
    "\n",
    "def get_relation_data(mention_pairs):\n",
    "    return mention_pairs[['is_exact_match', 'is_words_match', 'is_substring', 'is_abbreviation', 'is_appositive', 'is_nearest_candidate', 'sentence_distance', 'word_distance', 'markable_distance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soon-Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_11, text_21, prev_11, prev_21, next_11, next_21, syntactic_11, syntactic_21, is_singleton_11, is_singleton_21 = get_pair_data(pairs_soon.m1_id, pairs_soon.m2_id)\n",
    "relation_1 = get_relation_data(pairs_soon)\n",
    "label_soon = np.vstack(to_categorical(pairs_soon.is_coreference, num_classes=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words + Context + Syntactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_context_syntactic_model_builder = CoreferenceClassifierModelBuilder(\n",
    "    use_words_feature=True, \n",
    "    use_context_feature=True, \n",
    "    use_syntactic_feature=True,\n",
    "    use_relation_feature=True,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    syntactic_features_num=syntactic_11.shape[1],\n",
    "    relation_features_num=relation_1.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0508 12:37:42.085573 140479392503616 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:111: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0508 12:37:43.059978 140479392503616 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:135: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0508 12:37:43.103266 140479392503616 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1253: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0508 12:37:46.487572 140479392503616 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:123: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_1 = words_context_syntactic_model_builder.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5548/5548==============================] - 7s 1ms/sample - loss: 0.2786 - acc: 0.8861\n",
      "Epoch 2/5\n",
      "5548/5548==============================] - 4s 777us/sample - loss: 0.2016 - acc: 0.9030\n",
      "Epoch 3/5\n",
      "5548/5548==============================] - 4s 786us/sample - loss: 0.1768 - acc: 0.9297\n",
      "Epoch 4/5\n",
      "5548/5548==============================] - 4s 760us/sample - loss: 0.1598 - acc: 0.9427\n",
      "Epoch 5/5\n",
      "5548/5548==============================] - 4s 754us/sample - loss: 0.1405 - acc: 0.9531\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_1.fit([text_11, text_21, prev_11, prev_21, next_11, next_21, syntactic_11, syntactic_21, relation_1], label_soon, epochs=5)\n",
    "words_context_syntactic_model_1.save('models/coreference_classifiers/words_context_syntactic_soon_5.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5548/5548==============================] - 4s 771us/sample - loss: 0.1106 - acc: 0.9596\n",
      "Epoch 2/5\n",
      "5548/5548==============================] - 4s 771us/sample - loss: 0.0952 - acc: 0.9695\n",
      "Epoch 3/5\n",
      "5548/5548==============================] - 4s 784us/sample - loss: 0.1017 - acc: 0.9726\n",
      "Epoch 4/5\n",
      "5548/5548==============================] - 4s 777us/sample - loss: 0.0568 - acc: 0.9820\n",
      "Epoch 5/5\n",
      "5548/5548==============================] - 4s 787us/sample - loss: 0.0837 - acc: 0.9813\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_1.fit([text_11, text_21, prev_11, prev_21, next_11, next_21, syntactic_11, syntactic_21, relation_1], label_soon, epochs=5)\n",
    "words_context_syntactic_model_1.save('models/coreference_classifiers/words_context_syntactic_soon_10.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5548/5548==============================] - 4s 777us/sample - loss: 0.0795 - acc: 0.9845\n",
      "Epoch 2/10\n",
      "5548/5548==============================] - 4s 785us/sample - loss: 0.0387 - acc: 0.9885\n",
      "Epoch 3/10\n",
      "5548/5548==============================] - 4s 782us/sample - loss: 0.0354 - acc: 0.9906\n",
      "Epoch 4/10\n",
      "5548/5548==============================] - 4s 770us/sample - loss: 0.0648 - acc: 0.9841\n",
      "Epoch 5/10\n",
      "5548/5548==============================] - 4s 767us/sample - loss: 0.0426 - acc: 0.9888\n",
      "Epoch 6/10\n",
      "5548/5548==============================] - 4s 779us/sample - loss: 0.0362 - acc: 0.9926\n",
      "Epoch 7/10\n",
      "5548/5548==============================] - 4s 786us/sample - loss: 0.0332 - acc: 0.9913\n",
      "Epoch 8/10\n",
      "5548/5548==============================] - 4s 783us/sample - loss: 0.0440 - acc: 0.9865\n",
      "Epoch 9/10\n",
      "5548/5548==============================] - 4s 782us/sample - loss: 0.0351 - acc: 0.9957\n",
      "Epoch 10/10\n",
      "5548/5548==============================] - 4s 778us/sample - loss: 0.0454 - acc: 0.9895\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_1.fit([text_11, text_21, prev_11, prev_21, next_11, next_21, syntactic_11, syntactic_21, relation_1], label_soon, epochs=10)\n",
    "words_context_syntactic_model_1.save('models/coreference_classifiers/words_context_syntactic_soon_20.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gilang-Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_12, text_22, prev_12, prev_22, next_12, next_22, syntactic_12, syntactic_22, is_singleton_12, is_singleton_22 = get_pair_data(pairs_gilang.m1_id, pairs_gilang.m2_id)\n",
    "relation_2 = get_relation_data(pairs_gilang)\n",
    "label_gilang = np.vstack(to_categorical(pairs_gilang.is_coreference, num_classes=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words + Context + Syntactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_context_syntactic_model_2 = words_context_syntactic_model_builder.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36503/36503==============================] - 34s 921us/sample - loss: 0.0779 - acc: 0.9823\n",
      "Epoch 2/5\n",
      "36503/36503==============================] - 31s 843us/sample - loss: 0.0665 - acc: 0.9828\n",
      "Epoch 3/5\n",
      "36503/36503==============================] - 30s 835us/sample - loss: 0.0649 - acc: 0.9827\n",
      "Epoch 4/5\n",
      "36503/36503==============================] - 31s 846us/sample - loss: 0.0502 - acc: 0.9828\n",
      "Epoch 5/5\n",
      "36503/36503==============================] - 31s 849us/sample - loss: 0.0686 - acc: 0.9827\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_2.fit([text_12, text_22, prev_12, prev_22, next_12, next_22, syntactic_12, syntactic_22, relation_2], label_gilang, epochs=5)\n",
    "words_context_syntactic_model_2.save('models/coreference_classifiers/words_context_syntactic_gilang_5.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36503/36503==============================] - 31s 860us/sample - loss: 0.0469 - acc: 0.9829\n",
      "Epoch 2/5\n",
      "36503/36503==============================] - 31s 839us/sample - loss: 0.0452 - acc: 0.9829\n",
      "Epoch 3/5\n",
      "36503/36503==============================] - 31s 860us/sample - loss: 0.0400 - acc: 0.9829\n",
      "Epoch 4/5\n",
      "36503/36503==============================] - 31s 852us/sample - loss: 0.0399 - acc: 0.9831\n",
      "Epoch 5/5\n",
      "36503/36503==============================] - 30s 833us/sample - loss: 0.0649 - acc: 0.9824\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_2.fit([text_12, text_22, prev_12, prev_22, next_12, next_22, syntactic_12, syntactic_22, relation_2], label_gilang, epochs=5)\n",
    "words_context_syntactic_model_2.save('models/coreference_classifiers/words_context_syntactic_gilang_10.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36503/36503==============================] - 31s 841us/sample - loss: 0.0666 - acc: 0.9827\n",
      "Epoch 2/10\n",
      "36503/36503==============================] - 31s 850us/sample - loss: 0.0605 - acc: 0.9824\n",
      "Epoch 3/10\n",
      "36503/36503==============================] - 31s 861us/sample - loss: 0.0387 - acc: 0.9829\n",
      "Epoch 4/10\n",
      "36503/36503==============================] - 30s 826us/sample - loss: 0.0460 - acc: 0.9828\n",
      "Epoch 5/10\n",
      "36503/36503==============================] - 32s 885us/sample - loss: 0.0477 - acc: 0.9828\n",
      "Epoch 6/10\n",
      "36503/36503==============================] - 28s 778us/sample - loss: 0.0540 - acc: 0.9828\n",
      "Epoch 7/10\n",
      "36503/36503==============================] - 28s 777us/sample - loss: 0.1700 - acc: 0.9818\n",
      "Epoch 8/10\n",
      "36503/36503==============================] - 28s 778us/sample - loss: 0.0816 - acc: 0.9827\n",
      "Epoch 9/10\n",
      "36503/36503==============================] - 28s 778us/sample - loss: 0.0599 - acc: 0.9829\n",
      "Epoch 10/10\n",
      "36503/36503==============================] - 28s 780us/sample - loss: 0.0547 - acc: 0.9828\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_2.fit([text_12, text_22, prev_12, prev_22, next_12, next_22, syntactic_12, syntactic_22, relation_2], label_gilang, epochs=10)\n",
    "words_context_syntactic_model_2.save('models/coreference_classifiers/words_context_syntactic_gilang_20.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Budi-Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_13, text_23, prev_13, prev_23, next_13, next_23, syntactic_13, syntactic_23, is_singleton_13, is_singleton_23 = get_pair_data(pairs_budi.m1_id, pairs_budi.m2_id)\n",
    "relation_3 = get_relation_data(pairs_budi)\n",
    "label_budi = np.vstack(to_categorical(pairs_budi.is_coreference, num_classes=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words + Context + Syntactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_context_syntactic_model_3 = words_context_syntactic_model_builder.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "167420/167420==============================] - 145s 864us/sample - loss: 0.0528 - acc: 0.9880\n",
      "Epoch 2/5\n",
      "167420/167420==============================] - 142s 847us/sample - loss: 0.0527 - acc: 0.9882\n",
      "Epoch 3/5\n",
      "167420/167420==============================] - 145s 866us/sample - loss: 0.1048 - acc: 0.9883\n",
      "Epoch 4/5\n",
      "167420/167420==============================] - 143s 852us/sample - loss: 0.0906 - acc: 0.9882\n",
      "Epoch 5/5\n",
      "167420/167420==============================] - 143s 853us/sample - loss: 0.1092 - acc: 0.9881\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_3.fit([text_13, text_23, prev_13, prev_23, next_13, next_23, syntactic_13, syntactic_23, relation_3], label_budi, epochs=5)\n",
    "words_context_syntactic_model_3.save('models/coreference_classifiers/words_context_syntactic_budi_5.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "167420/167420==============================] - 143s 856us/sample - loss: 0.0931 - acc: 0.9883\n",
      "Epoch 2/5\n",
      "167420/167420==============================] - 147s 876us/sample - loss: 0.0659 - acc: 0.9884\n",
      "Epoch 3/5\n",
      "167420/167420==============================] - 143s 856us/sample - loss: 0.2270 - acc: 0.9881\n",
      "Epoch 4/5\n",
      "167420/167420==============================] - 142s 848us/sample - loss: 0.1253 - acc: 0.9883\n",
      "Epoch 5/5\n",
      "167420/167420==============================] - 144s 862us/sample - loss: 0.1784 - acc: 0.9883\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_3.fit([text_13, text_23, prev_13, prev_23, next_13, next_23, syntactic_13, syntactic_23, relation_3], label_budi, epochs=5)\n",
    "words_context_syntactic_model_3.save('models/coreference_classifiers/words_context_syntactic_budi_10.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "167420/167420==============================] - 144s 859us/sample - loss: 0.2594 - acc: 0.9880\n",
      "Epoch 2/10\n",
      "167420/167420==============================] - 146s 870us/sample - loss: 0.1272 - acc: 0.9882\n",
      "Epoch 4/10\n",
      "167420/167420==============================] - 144s 862us/sample - loss: 0.2033 - acc: 0.9883\n",
      "Epoch 5/10\n",
      "167420/167420==============================] - 144s 860us/sample - loss: 0.0612 - acc: 0.9883\n",
      "Epoch 6/10\n",
      "167420/167420==============================] - 143s 853us/sample - loss: 0.2545 - acc: 0.9884\n",
      "Epoch 7/10\n",
      "167420/167420==============================] - 146s 871us/sample - loss: 0.1343 - acc: 0.9883\n",
      "Epoch 8/10\n",
      "167420/167420==============================] - 143s 856us/sample - loss: 0.1077 - acc: 0.9884\n",
      "Epoch 9/10\n",
      "167420/167420==============================] - 144s 860us/sample - loss: 0.0988 - acc: 0.9883\n",
      "Epoch 10/10\n",
      "167420/167420==============================] - 146s 869us/sample - loss: 0.1814 - acc: 0.9883\n"
     ]
    }
   ],
   "source": [
    "words_context_syntactic_model_3.fit([text_13, text_23, prev_13, prev_23, next_13, next_23, syntactic_13, syntactic_23, relation_3], label_budi, epochs=10)\n",
    "words_context_syntactic_model_3.save('models/coreference_classifiers/words_context_syntactic_budi_20.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
